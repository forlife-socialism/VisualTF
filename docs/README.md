# VisualTF
可视化tensorflow学习记录，通过可视化方式观察梯度下降以及权重，偏差等变化情况。numpy,matploytlib

### [模拟线性回归](/linerRegression/linearReg.py)
![](https://media.giphy.com/media/JNPAKFsTx4PhRX1V65/giphy.gif)
#### [线性回归思想](/linerRegression/linerthink.py)
> 线性回归问题主要适用于所有数据可以模拟成一条函数曲线，而且一般的线性回归问题可以通过不断的缩减权重weight和偏置bias来进行模拟。
> h(x) = w*x + b 用于连续值的预测。
> w,b就是需要不断进行修正。一般都是通过梯度下降的方法将w,b进行修正。
> 梯度下降一般都是使用求导，不过有很多种优化方法。用来加快w,b修正的速度
> 一般的梯度下降方式有:
1. SGD
> 最简单的方式，就是测试数据分批进行神经网络计算。
2. Momentum
> 传统的W参数更新为: W += -Learning rate * dx
> Momentum 则是加上一个惯性，即m = b1 * m-Learning rate *dx W += m
3. AdaGrad
> 对学习率进行更新: v += dx^2 W += -Learning rate * dx/√v v算是一种惩罚措施，逼迫朝着正确的方向进行变化。
4. RMSProp
> 将AdaGrad和Momentum结合起来  v =  b1*v + (1-b1)*dx^2 W += -Learning rate * dx / √v
5. Adam
> m 
### [分类问题-mnist数据集识别](/classification/mnist.py)
> 分类问题从训练数据中训练模型之后将给出的事物进行准确的分类。
>本次例子主要是利用mnist数据集做0-9数据的识别。
>mnist数据集是0-9十个数字的70000张手写图片，其中的60000张图片为训练集。
>由于图片是灰度图片，所以使用[h,w]二维张量进行表示。如果是彩色图片还需要加上RGB通道即使用三维张量进行显示[h,w,c];
>每个位置存储的值为0~255，0为纯黑色，255为纯白色。当然如果使用matplotlib进行画图的时候可以使用cmap填充其他颜色。
#### 训练过程
> 由于使用的多输出节点，批量训练的方式。模型为: Y = X@W + b
>其中X 为向量[b,d_in],W为二维向量[d_in,d_out],偏置向量b为[d_out]。其中b为批次数量，d_in为单个输入样本的特征长度。
>d_out为输出长度。其中@为矩阵相乘，并且最后b应该需要自动扩展为[b,d_out]形状的向量。
1. 由于图片向量为二维张量，所以需要进行打平为[h*w]的形式。h*w即为输入样本的特征长度。
2. [one-hot独热编码](/classification/one-hot.py)
3. 误差计算。由于分类问题，一般情况下无法直接计算某个性能指标的导数。例如准确度。所以一般采用
一个平滑可导的代理函数。mnist识别数字，采用的是计算模型输出结果与真是结果之间的距离。损失函数采用均方差。
4. 激活函数，没有激活函数这样将是一个简单的线性函数模型，不足以表达复杂的数字图片识别。所以一般会加上激活函数，增强模型的表达能力，
强行“掰弯”模型。Sigmoid函数,ReLU函数。可以通过叠加激活函数而增强表达能力。
5. 搭建网络，使用框架的自动求导技术，人为监控某个参数和模型的关系比较困难。
### [输出层设计](/nn/output_layer_design.py)
1. 输出o属于整个实数空间，一般都是线性预测模型的输出结果。
此类一般不使用激活函数。且误差计算一般使用MSE直接使用输出结果进行计算。
2. 输出o落在[0,1]之间，这种常用于 图片生成或者二分类问题。一般使用Sigmoid函数
进行对结果的转换映射。最终使得结果归一化到[0,1]之间。
3. 输出o<sub>i</sub>属于[0,1]之间，且所有输出和为1，这种一般属于多分类问题，例如MNist手写字体识别。最后的输出结果是对应十个数字的概率，取最高的的一个。
一般使用SOftMax函数实现，将所有的输出进行归一并保证和为1.
4. 输出o<sub>i</sub>属于[-1,1]之间。使用tanh激活函数。
