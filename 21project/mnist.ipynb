{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train,label_train),(x_test,label_test) = mnist.load_data()\n",
    "print(x_train.shape)\n",
    "#这个label_train还没有经过独热编码\n",
    "print(label_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#将训练集中的自变量打平 从[60000,28,28]变为[60000,784]\n",
    "x_train,x_test = x_train.reshape([-1,784]),x_test.reshape([-1,784])\n",
    "x_train,x_test = x_train/255.,x_test/255.\n",
    "print(x_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "#构建逻辑回归所用参数\n",
    "W = tf.Variable(tf.ones([784,10]))\n",
    "b = tf.Variable(tf.ones([10]))\n",
    "print(W.shape)\n",
    "print(b.shape)\n",
    "#这样逻辑回归 XW+b = y 得到[n,10]大小的矩阵\n",
    "#n为每次处理批次大小"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#逻辑回归\n",
    "def logistic_regression(x):\n",
    "    return tf.matmul(x,W)+b\n",
    "#但是mnist手写字识别属于多分类问题，所以需要在逻辑回归上多加入一个softmax回归\n",
    "def predict_y(x):\n",
    "    return tf.nn.softmax(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((784,), ()), types: (tf.float64, tf.uint8)>\n",
      "<PrefetchDataset shapes: ((None, 784), (None,)), types: (tf.float64, tf.uint8)>\n"
     ]
    }
   ],
   "source": [
    "#构建每次处理的批次，这样可以根据每一次批次进行一次参数的调整，而不是等到全部训练之后才知道参数的正确率\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, label_train))\n",
    "print(train_data)\n",
    "train_data = train_data.repeat().shuffle(5000).batch(256).prefetch(1)\n",
    "print(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#交叉熵，多分类问题一般使用交叉熵作为损失函数\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    # 将真实标签进行独热编码\n",
    "    y_true = tf.one_hot(y_true, depth=10)\n",
    "    # 防止出现log0的错误\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "    # 计算交叉熵\n",
    "    #0表示每一列的元素相加，1表示每一行的元素相加\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred),1))\n",
    "#准确率 用来表示模型的准确程度\n",
    "def accuracy(y_pred, y_true):\n",
    "    # tf.argmax是从y_pred的每一行元素中找到最大的那个值的下标，之后和真实标签进行比较，这里可以不使用独热编码\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    #将[true,false....]转化为float64类型并相加求平均值得到准确率\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#优化，梯度加速器，使用学习率\n",
    "optimizer = tf.optimizers.SGD(0.01)\n",
    "#优化参数的过程，修改参数的值\n",
    "def run_optimize(x,y):\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = predict_y(logistic_regression(x))\n",
    "        loss = cross_entropy(pred, y)\n",
    "    # 计算梯度\n",
    "    gradients = g.gradient(loss, [W, b])\n",
    "    # 更新w和b的参数值\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10, loss: 0.522170, accuracy: 0.894531\n",
      "step: 20, loss: 0.566009, accuracy: 0.871094\n",
      "step: 30, loss: 0.520456, accuracy: 0.902344\n",
      "step: 40, loss: 0.650000, accuracy: 0.835938\n",
      "step: 50, loss: 0.622721, accuracy: 0.867188\n",
      "step: 60, loss: 0.594598, accuracy: 0.871094\n",
      "step: 70, loss: 0.599342, accuracy: 0.867188\n",
      "step: 80, loss: 0.497922, accuracy: 0.890625\n",
      "step: 90, loss: 0.581602, accuracy: 0.878906\n",
      "step: 100, loss: 0.620264, accuracy: 0.835938\n",
      "step: 110, loss: 0.573070, accuracy: 0.882812\n",
      "step: 120, loss: 0.642184, accuracy: 0.835938\n",
      "step: 130, loss: 0.631358, accuracy: 0.851562\n",
      "step: 140, loss: 0.645019, accuracy: 0.816406\n",
      "step: 150, loss: 0.676627, accuracy: 0.839844\n",
      "step: 160, loss: 0.578804, accuracy: 0.863281\n",
      "step: 170, loss: 0.559333, accuracy: 0.875000\n",
      "step: 180, loss: 0.613397, accuracy: 0.859375\n",
      "step: 190, loss: 0.575548, accuracy: 0.898438\n",
      "step: 200, loss: 0.549319, accuracy: 0.875000\n",
      "step: 210, loss: 0.480967, accuracy: 0.867188\n",
      "step: 220, loss: 0.531869, accuracy: 0.882812\n",
      "step: 230, loss: 0.564299, accuracy: 0.878906\n",
      "step: 240, loss: 0.520378, accuracy: 0.875000\n",
      "step: 250, loss: 0.598698, accuracy: 0.847656\n",
      "step: 260, loss: 0.535928, accuracy: 0.859375\n",
      "step: 270, loss: 0.516656, accuracy: 0.867188\n",
      "step: 280, loss: 0.577946, accuracy: 0.851562\n",
      "step: 290, loss: 0.590723, accuracy: 0.851562\n",
      "step: 300, loss: 0.541945, accuracy: 0.886719\n",
      "step: 310, loss: 0.559235, accuracy: 0.843750\n",
      "step: 320, loss: 0.559560, accuracy: 0.863281\n",
      "step: 330, loss: 0.561464, accuracy: 0.867188\n",
      "step: 340, loss: 0.527728, accuracy: 0.886719\n",
      "step: 350, loss: 0.631628, accuracy: 0.847656\n",
      "step: 360, loss: 0.546045, accuracy: 0.855469\n",
      "step: 370, loss: 0.495721, accuracy: 0.894531\n",
      "step: 380, loss: 0.596381, accuracy: 0.882812\n",
      "step: 390, loss: 0.481804, accuracy: 0.914062\n",
      "step: 400, loss: 0.612477, accuracy: 0.792969\n",
      "step: 410, loss: 0.563334, accuracy: 0.863281\n",
      "step: 420, loss: 0.430950, accuracy: 0.914062\n",
      "step: 430, loss: 0.494473, accuracy: 0.886719\n",
      "step: 440, loss: 0.532310, accuracy: 0.890625\n",
      "step: 450, loss: 0.503982, accuracy: 0.890625\n",
      "step: 460, loss: 0.530535, accuracy: 0.871094\n",
      "step: 470, loss: 0.562469, accuracy: 0.878906\n",
      "step: 480, loss: 0.452360, accuracy: 0.898438\n",
      "step: 490, loss: 0.552482, accuracy: 0.839844\n",
      "step: 500, loss: 0.516414, accuracy: 0.863281\n",
      "step: 510, loss: 0.496044, accuracy: 0.878906\n",
      "step: 520, loss: 0.648436, accuracy: 0.839844\n",
      "step: 530, loss: 0.480813, accuracy: 0.898438\n",
      "step: 540, loss: 0.546961, accuracy: 0.855469\n",
      "step: 550, loss: 0.538917, accuracy: 0.843750\n",
      "step: 560, loss: 0.571720, accuracy: 0.851562\n",
      "step: 570, loss: 0.469666, accuracy: 0.890625\n",
      "step: 580, loss: 0.532152, accuracy: 0.882812\n",
      "step: 590, loss: 0.484520, accuracy: 0.890625\n",
      "step: 600, loss: 0.622487, accuracy: 0.851562\n",
      "step: 610, loss: 0.477179, accuracy: 0.882812\n",
      "step: 620, loss: 0.481749, accuracy: 0.894531\n",
      "step: 630, loss: 0.639975, accuracy: 0.820312\n",
      "step: 640, loss: 0.587885, accuracy: 0.855469\n",
      "step: 650, loss: 0.600937, accuracy: 0.851562\n",
      "step: 660, loss: 0.510320, accuracy: 0.886719\n",
      "step: 670, loss: 0.470887, accuracy: 0.894531\n",
      "step: 680, loss: 0.504669, accuracy: 0.875000\n",
      "step: 690, loss: 0.433563, accuracy: 0.894531\n",
      "step: 700, loss: 0.460474, accuracy: 0.882812\n",
      "step: 710, loss: 0.420858, accuracy: 0.902344\n",
      "step: 720, loss: 0.483226, accuracy: 0.890625\n",
      "step: 730, loss: 0.518424, accuracy: 0.863281\n",
      "step: 740, loss: 0.455830, accuracy: 0.894531\n",
      "step: 750, loss: 0.572352, accuracy: 0.835938\n",
      "step: 760, loss: 0.511419, accuracy: 0.882812\n",
      "step: 770, loss: 0.507693, accuracy: 0.875000\n",
      "step: 780, loss: 0.503464, accuracy: 0.878906\n",
      "step: 790, loss: 0.469947, accuracy: 0.863281\n",
      "step: 800, loss: 0.476529, accuracy: 0.859375\n",
      "step: 810, loss: 0.559074, accuracy: 0.863281\n",
      "step: 820, loss: 0.445763, accuracy: 0.906250\n",
      "step: 830, loss: 0.563793, accuracy: 0.843750\n",
      "step: 840, loss: 0.506375, accuracy: 0.878906\n",
      "step: 850, loss: 0.515869, accuracy: 0.871094\n",
      "step: 860, loss: 0.519094, accuracy: 0.886719\n",
      "step: 870, loss: 0.479050, accuracy: 0.875000\n",
      "step: 880, loss: 0.533018, accuracy: 0.863281\n",
      "step: 890, loss: 0.496901, accuracy: 0.859375\n",
      "step: 900, loss: 0.452853, accuracy: 0.894531\n",
      "step: 910, loss: 0.467409, accuracy: 0.863281\n",
      "step: 920, loss: 0.438225, accuracy: 0.925781\n",
      "step: 930, loss: 0.464527, accuracy: 0.914062\n",
      "step: 940, loss: 0.455651, accuracy: 0.890625\n",
      "step: 950, loss: 0.461899, accuracy: 0.898438\n",
      "step: 960, loss: 0.521022, accuracy: 0.886719\n",
      "step: 970, loss: 0.453525, accuracy: 0.882812\n",
      "step: 980, loss: 0.552500, accuracy: 0.863281\n",
      "step: 990, loss: 0.469296, accuracy: 0.886719\n",
      "step: 1000, loss: 0.442471, accuracy: 0.898438\n",
      "<tf.Variable 'Variable:0' shape=(784, 10) dtype=float32, numpy=\n",
      "array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)> <tf.Variable 'Variable:0' shape=(10,) dtype=float32, numpy=\n",
      "array([0.93241084, 1.1402669 , 0.9665578 , 0.9531013 , 1.0429226 ,\n",
      "       1.1119792 , 0.9845638 , 1.0809453 , 0.81400084, 0.97325516],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "#开始训练\n",
    "#训练1000次，每次批次大小为256\n",
    "x_test = tf.cast(x_test,tf.float32)\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(1000), 1):\n",
    "    batch_x = tf.cast(batch_x,tf.float32)\n",
    "    run_optimize(batch_x, batch_y)\n",
    "    pred = predict_y(logistic_regression(batch_x))\n",
    "    acc = accuracy(pred, batch_y)\n",
    "    if step % 10 == 0:\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n",
    "        # pred = logistic_regression(x_test)\n",
    "        # print(\"Test Accuracy: %f\" % accuracy(pred, label_test))\n",
    "print(W,b)\n",
    "#正确率89%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}